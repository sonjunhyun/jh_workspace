{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_OzRnst_HJJu"
      },
      "source": [
        "# 토픽모델링 (Topic Modeling)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Xh94rk2QHMxR"
      },
      "source": [
        "# 0 환경"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-hrtKncHPL4"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get install -y fonts-nanum\n",
        "!sudo fc-cache -fv\n",
        "!rm ~/.cache/matplotlib -rf\n",
        "\n",
        "# 런타임 재시작 !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMcxNcl_V0sM"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.rc('font', family='NanumBarunGothic')\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aki-vwzBSrE",
        "outputId": "a29f641f-fae5-4ad9-fb9d-bc71b8ec1a79"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Jyp5nHXQsXez"
      },
      "source": [
        "# 1 잠재의미분석 (Latent Semantic Analysis LSA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFgOmDD84TkZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.decomposition import randomized_svd\n",
        "\n",
        "class LSA :\n",
        "    def __init__(self, doc_ls, topic_num):\n",
        "        self.doc_ls = doc_ls\n",
        "        self.topic_num = topic_num\n",
        "        self.term2idx, self.idx2term = self.toIdxDict(' '.join(doc_ls).split())\n",
        "        self.doc2idx, self.idx2doc = self.toIdxDict(doc_ls)\n",
        "\n",
        "        self.tdm = self.TDM(doc_ls)\n",
        "        self.U, self.s, self.VT = self.SVD(self.tdm)\n",
        "\n",
        "\n",
        "        self.term_mat = self.TermVectorMatrix(self.U, self.s, topic_num)\n",
        "        self.doc_mat = self.DocVectorMatrix(self.s, self.VT, topic_num)\n",
        "        self.term_doc_mat = self.TermDocVectorMatrix(self.U, self.s, self.VT, topic_num)\n",
        "\n",
        "        self.term_sim = self.TermSimilarityMatrix(np.asarray(self.term_mat).T)\n",
        "        self.doc_sim = self.DocSimilarityMartrix(np.asarray(self.doc_mat).T)\n",
        "\n",
        "    # 리스트내 값을 index로 변환하는 dict과\n",
        "    # index를 리스트내 값으로 변환하는 dict\n",
        "    def toIdxDict(self, ls) :\n",
        "        any2idx = defaultdict(lambda : len(any2idx))\n",
        "        idx2any = defaultdict()\n",
        "\n",
        "        for item in ls:\n",
        "            any2idx[item]\n",
        "            idx2any[any2idx[item]] = item\n",
        "\n",
        "        print(idx2any)\n",
        "        return any2idx, idx2any\n",
        "\n",
        "    def TDM(self, doc_ls):\n",
        "        # 행(토큰크기), 열(문서갯수)로 TDM 생성\n",
        "        tdm = np.zeros([len(self.term2idx.keys()), len(doc_ls)])\n",
        "\n",
        "        for doc_idx, doc in enumerate(doc_ls) :\n",
        "            for term in doc.split() :\n",
        "              #등장한 단어를 dictionary에서 위치를 탐색하여 빈도수 세기\n",
        "              tdm[self.term2idx[term], doc_idx] += 1\n",
        "        self.tdm = tdm\n",
        "\n",
        "        return tdm\n",
        "\n",
        "    # 특이값 분해\n",
        "    def SVD(self, tdm):\n",
        "        U, s, VT = randomized_svd(tdm,\n",
        "                                  n_components=self.topic_num,\n",
        "                                  n_iter=10,\n",
        "                                  random_state=None)\n",
        "\n",
        "        U, s, VT = np.linalg.svd(tdm, full_matrices=True)\n",
        "        return U, s, VT\n",
        "\n",
        "    # 토픽별 주요 키워드 출력\n",
        "    def TopicModeling(self, count = 3) :\n",
        "        topic_num = self.topic_num\n",
        "\n",
        "        for i in range(topic_num) :\n",
        "            score = self.U[:,i:i+1].T\n",
        "            print(score)\n",
        "            sorted_index = np.flip(np.argsort(-score),0)\n",
        "\n",
        "            a = []\n",
        "            for j in sorted_index[0,: count] :\n",
        "                a.append((self.idx2term[j], score[0,j].round(3)))\n",
        "\n",
        "            print(\"Topic {} - {}\".format(i+1,a ))\n",
        "\n",
        "    def vectorSimilarity(self, matrix) :\n",
        "        cos_sim = cosine_similarity(matrix, matrix)\n",
        "        return cos_sim\n",
        "\n",
        "    # 키워드를 입력했을 때 단어 벡터 반환\n",
        "    def GetTermVector(self, term):\n",
        "        vec = self.term_mat[self.term2idx[term]:self.term2idx[term]+1,:]\n",
        "        print('{} = {}'.format(term, vec))\n",
        "        return vec\n",
        "\n",
        "    # 문서를 입력했을 때 문서 벡터 반환\n",
        "    def GetDocVector(self, doc):\n",
        "        vec = self.doc_mat.T[self.doc2idx[doc]:self.doc2idx[doc]+1,:]\n",
        "        print('{} = {}'.format(doc, vec))\n",
        "        return vec\n",
        "\n",
        "    def Compression(self, round_num=0) :\n",
        "        print(self.tdm)\n",
        "        print(self.term_doc_mat.round(round_num))\n",
        "\n",
        "    def TermVectorMatrix(self, u, s, topic_num):\n",
        "        term_mat = np.matrix(u[:, :topic_num])# * np.diag(s[:topic_num])\n",
        "        return term_mat\n",
        "\n",
        "    def DocVectorMatrix(self, s, vt, topic_num):\n",
        "        doc_mat = np.matrix(vt[:topic_num,:])\n",
        "        return doc_mat\n",
        "\n",
        "    def TermDocVectorMatrix(self, u, s, vt, topic_num):\n",
        "        term_doc_mat = np.matrix(u[:, :topic_num]) * np.diag(s[:topic_num])  * np.matrix(vt[:topic_num,:])\n",
        "        return term_doc_mat\n",
        "\n",
        "    def TermSimilarityMatrix(self, termVectorMatrix):\n",
        "        return self.vectorSimilarity(termVectorMatrix.T)\n",
        "\n",
        "    def GetTermSimilarity(self, term1, term2):\n",
        "        sim = self.term_sim[self.term2idx[term1], self.term2idx[term2]]\n",
        "        print(\"({},{}) term similarity = {}\".format(term1, term2, sim))\n",
        "        return sim\n",
        "\n",
        "    def DocSimilarityMartrix(self,docVectorMatrix):\n",
        "        return self.vectorSimilarity(docVectorMatrix)\n",
        "\n",
        "    def GetDocSimilarity(self, doc1, doc2):\n",
        "        sim = self.doc_sim[self.doc2idx[doc1], self.doc2idx[doc2]]\n",
        "        print(\"('{}','{}') doc similarity = {}\".format(doc1, doc2, sim))\n",
        "        return sim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "on3YdPBZp2te",
        "outputId": "43832bc8-9416-4930-d788-d30764b7ec0b"
      },
      "outputs": [],
      "source": [
        "doc_ls = [\n",
        "    '바나나 사과 포도 포도 짜장면',\n",
        "    '사과 포도',\n",
        "    '포도 바나나',\n",
        "    '짜장면 짬뽕 탕수육',\n",
        "    '볶음밥 탕수육',\n",
        "    '짜장면 짬뽕',\n",
        "    '라면 스시',\n",
        "    '스시 짜장면',\n",
        "    '가츠동 스시 소바',\n",
        "    '된장찌개 김치찌개 김치',\n",
        "    '김치 된장 짜장면',\n",
        "    '비빔밥 김치'\n",
        "]\n",
        "lsa = LSA(doc_ls, 4)\n",
        "print('== 토픽 모델링 ==')\n",
        "lsa.TopicModeling(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jMEhMzVdBIX",
        "outputId": "1f33450b-a621-48cb-d395-6fd2045532ad"
      },
      "outputs": [],
      "source": [
        "lsa.tdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcS6Ug9hbuAv"
      },
      "outputs": [],
      "source": [
        "np.linalg.svd?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqZnr8SX86qt",
        "outputId": "2cb24c69-46e1-44ee-a323-03118c5fac7c"
      },
      "outputs": [],
      "source": [
        "doc_ls = [\n",
        "    '바나나 사과 포도 포도 짜장면',\n",
        "    '사과 포도',\n",
        "    '포도 바나나',\n",
        "    '짜장면 짬뽕 탕수육',\n",
        "    '볶음밥 탕수육',\n",
        "    '짜장면 짬뽕',\n",
        "    '라면 스시',\n",
        "    '스시 짜장면',\n",
        "    '가츠동 스시 소바',\n",
        "    '된장찌개 김치찌개 김치',\n",
        "    '김치 된장 짜장면',\n",
        "    '비빔밥 김치'\n",
        "]\n",
        "lsa = LSA(doc_ls, 4)\n",
        "X = lsa.TDM(doc_ls)\n",
        "print('== 토픽 모델링 ==')\n",
        "lsa.TopicModeling(4)\n",
        "print('\\n== 단어 벡터 ==')\n",
        "lsa.GetTermVector('사과')\n",
        "lsa.GetTermVector('짜장면')\n",
        "print('\\n== 단어 유사도 ==')\n",
        "lsa.GetTermSimilarity('사과','바나나')\n",
        "lsa.GetTermSimilarity('사과','짜장면')\n",
        "lsa.GetTermSimilarity('포도','짜장면')\n",
        "lsa.GetTermSimilarity('사과','스시')\n",
        "print('\\n== 문서 벡터 ==')\n",
        "lsa.GetDocVector('사과 포도')\n",
        "lsa.GetDocVector('짜장면 짬뽕')\n",
        "print('\\n== 문서 유사도 ==')\n",
        "lsa.GetDocSimilarity('사과 포도', '포도 바나나')\n",
        "lsa.GetDocSimilarity('사과 포도', '라면 스시')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "31Kn3iEWLDz9"
      },
      "source": [
        "## 1.2 sklearn 활용"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kaaw49O5mqec"
      },
      "source": [
        "### 1) 토픽모델링"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LVVXaR4EJCZ"
      },
      "outputs": [],
      "source": [
        "doc_ls = [\n",
        "    '바나나 사과 포도 포도 짜장면',\n",
        "    '사과 포도',\n",
        "    '포도 바나나',\n",
        "    '짜장면 짬뽕 탕수육',\n",
        "    '볶음밥 탕수육',\n",
        "    '짜장면 짬뽕',\n",
        "    '라면 스시',\n",
        "    '스시 짜장면',\n",
        "    '가츠동 스시 소바',\n",
        "    '된장찌개 김치찌개 김치',\n",
        "    '김치 된장 짜장면',\n",
        "    '비빔밥 김치'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOPy5VKzKr8_"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "n_topic= 4\n",
        "\n",
        "tfidf_vect = TfidfVectorizer(max_features= 1000, max_df = 0.5, smooth_idf=True)\n",
        "tfidf = tfidf_vect.fit_transform(doc_ls)\n",
        "svd = TruncatedSVD(n_components=n_topic, algorithm='randomized', n_iter=100)\n",
        "u_sigma = svd.fit_transform(tfidf)\n",
        "svd.components_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fkHor8JeRuFz"
      },
      "outputs": [],
      "source": [
        "vocab = tfidf_vect.get_feature_names() # 단어 집합. 1,000개의 단어가 저장됨.\n",
        "n = 3\n",
        "for idx, topic in enumerate(svd.components_):\n",
        "    print(\"Topic %d:\" % (idx), [(vocab[i], topic[i].round(5)) for i in topic.argsort()[:-n - 1:-1]])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Cj3fMxdamzB6"
      },
      "source": [
        "### 2) 단어벡터"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQvD_3Lagat8"
      },
      "outputs": [],
      "source": [
        "svd.components_.T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2VjqPqSkOk4"
      },
      "outputs": [],
      "source": [
        "# 단어벡터\n",
        "for i in range(len(vocab)) :\n",
        "    print(\"{} : {}\".format(vocab[i], svd.components_.T[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdzmkhP8ELav"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "\n",
        "def calc_similarity_matrix(vectors) :\n",
        "    def cosine_similarity(a, b) :\n",
        "        return dot(a, b)/(norm(a)*norm(b))\n",
        "\n",
        "    n_word = len(vectors)\n",
        "    similarity_matrix = np.zeros((n_word, n_word))\n",
        "\n",
        "    for i in range(n_word) :\n",
        "        for j in range(i, n_word) :\n",
        "            similarity_matrix[j, i] = cosine_similarity(vectors[i], vectors[j]).round(4)\n",
        "\n",
        "    return similarity_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QzEGCRDzB_CU"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "def visualize_similarity(similarity_matrix) :\n",
        "    uniform_data = similarity_matrix\n",
        "    mask = np.triu(np.ones_like(similarity_matrix, dtype=np.bool))\n",
        "    plt.rcParams['figure.figsize'] = [8, 6]\n",
        "    ax = sns.heatmap(uniform_data, mask=mask, #xticklabels=features, yticklabels=features,\n",
        "                    annot=True, fmt=\".2f\",annot_kws={'size':8}, cmap='coolwarm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWJI_8rbBzAL"
      },
      "outputs": [],
      "source": [
        "word_vectors = svd.components_.T\n",
        "word_similarity_matrix = calc_similarity_matrix(word_vectors)\n",
        "visualize_similarity(word_similarity_matrix)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0yfe991tm1qa"
      },
      "source": [
        "### 3) 문서벡터"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49wUP0VdkQVD"
      },
      "outputs": [],
      "source": [
        "doc_vectors = u_sigma/svd.singular_values_\n",
        "doc_similarity_matrix = calc_similarity_matrix(doc_vectors)\n",
        "visualize_similarity(doc_similarity_matrix)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uGFHkelP2Upp"
      },
      "source": [
        "### 4) 벡터 시각화\n",
        "\n",
        "- manifold.TSNE() : t-SNE(t분포 Stochastic Neighbor Embedding) 차원 축소 기법의 하나"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUl3WoSi2vw3"
      },
      "outputs": [],
      "source": [
        "vectors = word_vectors\n",
        "labels = tfidf_vect.get_feature_names()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVezoaECoPzO"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import numpy as np\n",
        "\n",
        "def visualize_vectors(vectors, labels):\n",
        "    tsne = TSNE(n_components=2, random_state=0, n_iter=10000, perplexity=2)\n",
        "    np.set_printoptions(suppress=True)\n",
        "    T = tsne.fit_transform(vectors)\n",
        "    #labels = vocab\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.scatter(T[:, 0], T[:, 1], c='orange', edgecolors='r')\n",
        "    for label, x, y in zip(labels, T[:, 0], T[:, 1]):\n",
        "        plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ufmu2SzjDWJj"
      },
      "outputs": [],
      "source": [
        "visualize_vectors(vectors, labels)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4EpgqX0rVPjM"
      },
      "source": [
        "### 5) 파이프라인 사용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qn6YnSohVTdo"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "\n",
        "def my_tokenizer(text):\n",
        "    return [w for w in text.split() if len(w) > 1]\n",
        "\n",
        "lsa_pipeline = Pipeline([\n",
        "    ('vect', CountVectorizer(tokenizer = my_tokenizer)),\n",
        "    ('tfidf', TfidfTransformer(smooth_idf=True)),\n",
        "    ('lsa', TruncatedSVD(n_components=n_topic, algorithm='randomized', n_iter=100)),\n",
        "])\n",
        "\n",
        "lsa_pipeline.fit(doc_ls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJjRa7wictNS"
      },
      "outputs": [],
      "source": [
        "lsa = lsa_pipeline.named_steps['lsa']\n",
        "count_vect = lsa_pipeline.named_steps['vect']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYzHYwd_cxEh"
      },
      "outputs": [],
      "source": [
        "vocab = count_vect.get_feature_names() # 단어 집합. 1,000개의 단어가 저장됨.\n",
        "\n",
        "def get_topics(components, feature_names, n=3):\n",
        "    for idx, topic in enumerate(components):\n",
        "        print(\"Topic %d:\" % (idx), [(feature_names[i], topic[i].round(5)) for i in topic.argsort()[:-n - 1:-1]])\n",
        "get_topics(lsa.components_,vocab)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QiJXdInJLn-4"
      },
      "source": [
        "## 1.3 gensim 활용"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "q5R7Z3VULtQT"
      },
      "source": [
        "### 1) 토픽모델링"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lz82ZZpkLr9I"
      },
      "outputs": [],
      "source": [
        "docs = [\n",
        "    '바나나 사과 포도 포도',\n",
        "    '사과 포도',\n",
        "    '포도 바나나',\n",
        "    '짜장면 짬뽕 탕수욕',\n",
        "    '볶음밥 탕수욕',\n",
        "    '짜장면 짬뽕',\n",
        "    '라면 스시',\n",
        "    '스시',\n",
        "    '가츠동 스시 소바',\n",
        "    '된장찌개 김치찌개 김치',\n",
        "    '김치 된장',\n",
        "    '비빔밥 김치'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjZIWeWuL1NT"
      },
      "outputs": [],
      "source": [
        "doc_ls = [doc.split() for doc in docs]\n",
        "doc_ls[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCcg7TSHL5ZR"
      },
      "outputs": [],
      "source": [
        "from gensim import corpora\n",
        "from gensim.models import LsiModel\n",
        "from gensim.models import TfidfModel\n",
        "\n",
        "n_dim = 4\n",
        "\n",
        "id2word = corpora.Dictionary(doc_ls)\n",
        "corpus_TDM = [id2word.doc2bow(text) for text in doc_ls]\n",
        "tfidf = TfidfModel(corpus_TDM) #train\n",
        "corpus_TFIDF = tfidf[corpus_TDM] #predict\n",
        "model_LSA = LsiModel(corpus_TFIDF, id2word=id2word, num_topics=n_dim)\n",
        "\n",
        "for top in model_LSA.print_topics(n_dim, 3):\n",
        "    print(top)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-kX7e6Hm38H"
      },
      "outputs": [],
      "source": [
        "model_LSA.projection.u"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QinBYNCcndqZ"
      },
      "outputs": [],
      "source": [
        "from gensim.matutils import sparse2full\n",
        "corpus_VT = model_LSA[corpus_TDM]\n",
        "VT = [sparse2full(doc_vector, n_dim).tolist() for doc_vector in corpus_VT]\n",
        "VT"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OcR4bsoaL_V-"
      },
      "source": [
        "### 2) 단어벡터"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qe2_eReNMRta"
      },
      "outputs": [],
      "source": [
        "for i in id2word.keys() :\n",
        "    print(\"{} : {}\".format(id2word[i], model_LSA.projection.u[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvsgIQEOC2Fv"
      },
      "outputs": [],
      "source": [
        "word_vectors = model_LSA.projection.u\n",
        "word_similarity_matrix = calc_similarity_matrix(word_vectors)\n",
        "visualize_similarity(word_similarity_matrix)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "EAX85JKnMDOR"
      },
      "source": [
        "### 3) 문서벡터"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwLmgBaBMZmQ"
      },
      "outputs": [],
      "source": [
        "from gensim.matutils import sparse2full\n",
        "corpus_V = model_LSA[corpus_TDM]\n",
        "V = [sparse2full(doc_vector, n_dim).tolist() for doc_vector in corpus_VT]\n",
        "\n",
        "doc_vectors = V\n",
        "doc_similarity_matrix = calc_similarity_matrix(doc_vectors)\n",
        "visualize_similarity(doc_similarity_matrix)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "l-nbVVMzMGEo"
      },
      "source": [
        "### 4) 벡터시각화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7AA98BMjDdpn"
      },
      "outputs": [],
      "source": [
        "vectors = word_vectors\n",
        "labels = [k for k in id2word.keys()]\n",
        "\n",
        "visualize_vectors(vectors, labels)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ATfiKPg2wHkQ"
      },
      "source": [
        "# 2 잠재디리클레할당(LDA, Latent Dirichlet Allocation)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kTdsTBi5qO6x"
      },
      "source": [
        "## 2.1 직접구현"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "adoVt_1RNJB6"
      },
      "source": [
        "### 1) 직접 구현"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qxl6Bat79QQk"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gIJ4GNhEW0v"
      },
      "outputs": [],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "doc_ls = [\n",
        "    \"Cute kitty\",\n",
        "    \"Eat rice or cake\",\n",
        "    \"Kitty and hamster\",\n",
        "    \"Eat bread\",\n",
        "    \"Rice, bread and cake\",\n",
        "    \"Cute hamster eats bread and cake\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 랜덤하게 주제 할당\n",
        "\n",
        "# Document, Token 별 Topic 카운트\n",
        "\n",
        "# 재할당 과정 반복\n",
        "\n",
        "# 토픽 별 단어 추출"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0sfF7Quwrv9m"
      },
      "outputs": [],
      "source": [
        "lda = LDA(doc_ls, 2)\n",
        "lda.TopicModeling(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "RS7TS1QP9Q7e"
      },
      "source": [
        "## 2.2  sklearn 활용"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vwuy0ZcyNhfa"
      },
      "source": [
        "### 1) 토픽모델링 (파이프라인 미사용)\n",
        "\n",
        "- decomposition.LatentDirichletAllocation() : LDA 모델"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pX2qNArwVPbY"
      },
      "outputs": [],
      "source": [
        "!pip install pyLDAvis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SaZrFgPaOuqJ"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "#뉴스 다운로드 및 전처리\n",
        "def get_news(apply_split=True) :\n",
        "    #20newsgroup 다운로드\n",
        "    dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
        "    documents = dataset.data\n",
        "\n",
        "    news_df = pd.DataFrame({'document':documents})\n",
        "    news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \") # 특수 문자 제거\n",
        "    news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3])) # 길이가 3이하인 단어는 제거 (길이가 짧은 단어 제거)\n",
        "    news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())# 전체 단어에 대한 소문자 변환\n",
        "    tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split()) # 토큰화\n",
        "\n",
        "    stop_words = stopwords.words('english') # NLTK 불용어 조회\n",
        "\n",
        "    if apply_split :\n",
        "        return tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n",
        "    else :\n",
        "        return tokenized_doc.apply(lambda x: ' '.join([item for item in x if item not in stop_words]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cz82Rew1s9kF"
      },
      "outputs": [],
      "source": [
        "#공백으로 토큰 분리\n",
        "def my_tokenizer(text):\n",
        "    return text.split()\n",
        "\n",
        "tokenized_docs = get_news(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIuciWdLLeuf"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "tfidf_vect = TfidfVectorizer(tokenizer = my_tokenizer)\n",
        "tfidf = tfidf_vect.fit_transform(tokenized_docs)\n",
        "lda = LatentDirichletAllocation(n_components=20,\n",
        "                                max_iter=20,\n",
        "                                learning_method='online',\n",
        "                                random_state=100)\n",
        "\n",
        "lda_output = lda.fit_transform(tfidf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyAE2FJjI1CV"
      },
      "outputs": [],
      "source": [
        "#!pip install pyLDAvis\n",
        "import pyLDAvis\n",
        "import pyLDAvis.sklearn\n",
        "\n",
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.sklearn.prepare(lda, tfidf, tfidf_vect, mds='tsne')\n",
        "pyLDAvis.display(vis)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iTfiF5E7VDk3"
      },
      "source": [
        "### 2) 토픽모델링 (파이프라인 사용)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppgErMneHMhs"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "tokenized_docs = get_news(False)\n",
        "#파이프라이구성\n",
        "lda_pipeline = Pipeline([\n",
        "    ('tfidf_vect', TfidfVectorizer(tokenizer = my_tokenizer)),\n",
        "    ('lda', LatentDirichletAllocation(n_components=20,\n",
        "                                      max_iter=20,\n",
        "                                      learning_method='online',\n",
        "                                      random_state=100))])\n",
        "\n",
        "lda_pipeline.fit(tokenized_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPcr3FOGtj3Z"
      },
      "outputs": [],
      "source": [
        "!pip install pyLDAvis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfenOQXhU1q_"
      },
      "outputs": [],
      "source": [
        "#!pip install pyLDAvis\n",
        "import pyLDAvis\n",
        "import pyLDAvis.sklearn\n",
        "\n",
        "tfidf_vect = lda_pipeline.named_steps['tfidf_vect']\n",
        "tfidf = tfidf_vect.fit_transform(tokenized_docs)\n",
        "lda = lda_pipeline.named_steps['lda']\n",
        "\n",
        "vis = pyLDAvis.sklearn.prepare(lda, tfidf, tfidf_vect, mds='tsne')\n",
        "pyLDAvis.display(vis)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-Cdpwhk7N80Y"
      },
      "source": [
        "### 3) 하이퍼파라미터 튜닝"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxA8XpMQHLHF"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def tuning_hyperparams(train_data, n_jobs=1) :\n",
        "    lda_pipeline = Pipeline([\n",
        "        ('tfidf_vect', TfidfVectorizer(tokenizer = my_tokenizer)),\n",
        "        ('lda', LatentDirichletAllocation(max_iter=10, random_state=100))\n",
        "    ])\n",
        "\n",
        "    search_params = {\n",
        "        'tfidf_vect__ngram_range': [(1, 1), (1, 2)],\n",
        "        'tfidf_vect__use_idf': (True, False),\n",
        "        'lda__n_components': [10, 20]\n",
        "    }\n",
        "\n",
        "    gs_lda = GridSearchCV(lda_pipeline, search_params, n_jobs=n_jobs)\n",
        "    gs_lda = gs_lda.fit(train_data)\n",
        "    print(\"Best score: {0}\".format(gs_lda.best_score_))\n",
        "    print(\"Best parameters set:\")\n",
        "    best_parameters = gs_lda.best_estimator_.get_params()\n",
        "    for param_name in sorted(list(best_parameters.keys())):\n",
        "        print(\"\\t{0}: {1}\".format(param_name, best_parameters[param_name]))\n",
        "    return gs_lda.best_estimator_\n",
        "\n",
        "lda_pipeline = tuning_hyperparams(tokenized_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x16CyCWMXj8R"
      },
      "outputs": [],
      "source": [
        "#!pip install pyLDAvis\n",
        "import pyLDAvis\n",
        "import pyLDAvis.sklearn\n",
        "\n",
        "tfidf_vect = lda_pipeline.named_steps['tfidf_vect']\n",
        "tfidf = tfidf_vect.fit_transform(tokenized_docs)\n",
        "lda = lda_pipeline.named_steps['lda']\n",
        "\n",
        "vis = pyLDAvis.sklearn.prepare(lda, tfidf, tfidf_vect, mds='tsne')\n",
        "pyLDAvis.display(vis)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xf_J97e1NsTl"
      },
      "source": [
        "## 2.3 gensim 활용"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mbB7yMnRNvB6"
      },
      "source": [
        "### 1) 토픽모델링"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0hifkL58rrB"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-FAziaL9KJ6"
      },
      "outputs": [],
      "source": [
        "from gensim import corpora\n",
        "from gensim.models import LdaModel, TfidfModel\n",
        "\n",
        "tokenized_docs = get_news()\n",
        "id2word = corpora.Dictionary(tokenized_docs)\n",
        "corpus_TDM = [id2word.doc2bow(doc) for doc in tokenized_docs]\n",
        "tfidf = TfidfModel(corpus_TDM)\n",
        "corpus_TFIDF = tfidf[corpus_TDM]\n",
        "\n",
        "n = 20\n",
        "lda = LdaModel(corpus=corpus_TFIDF,\n",
        "               id2word=id2word,\n",
        "               num_topics=n,\n",
        "               random_state=100)\n",
        "\n",
        "for t in lda.print_topics():\n",
        "    print(t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shGD7kDtDF7_"
      },
      "outputs": [],
      "source": [
        "corpus_TDM = [id2word.doc2bow(doc) for doc in tokenized_docs]\n",
        "tfidf = TfidfModel(corpus_TDM)\n",
        "corpus_TFIDF = tfidf[corpus_TDM]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42GDOBDWYR0D"
      },
      "outputs": [],
      "source": [
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "\n",
        "pyLDAvis.enable_notebook()\n",
        "vis = gensimvis.prepare(lda, corpus_TFIDF, id2word, mds='tsne')\n",
        "pyLDAvis.display(vis)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jJ_1fTwuN4Vd"
      },
      "source": [
        "### 2) 하이퍼파라미터 튜닝\n",
        "\n",
        "- models.coherencemodel.CoherenceModel() : LDA에서 최적 토픽 개수를 추출하는 모델 (토픽의 응집력 계산)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lc9UOlhKPDj8"
      },
      "outputs": [],
      "source": [
        "from gensim import corpora\n",
        "from gensim.models import LdaModel, TfidfModel\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "\n",
        "def compute_coherence_perplexity(tokenized_docs, end, start=2, step=3) :\n",
        "    id2word = corpora.Dictionary(tokenized_docs)\n",
        "    corpus_TDM = [id2word.doc2bow(doc) for doc in tokenized_docs]\n",
        "    tfidf = TfidfModel(corpus_TDM)\n",
        "    corpus_TFIDF = tfidf[corpus_TDM]\n",
        "\n",
        "    coherence_values = []\n",
        "    perplexity_values = []\n",
        "    model_list = []\n",
        "    topic_n_list = []\n",
        "\n",
        "    for num_topics in range(start, end, step):\n",
        "        model = LdaModel(corpus_TFIDF, num_topics=num_topics, id2word = id2word)\n",
        "\n",
        "        model_list.append(model)\n",
        "        coherencemodel = CoherenceModel(model=model,\n",
        "                                        texts=tokenized_docs,\n",
        "                                        dictionary=id2word,\n",
        "                                        coherence='c_v')\n",
        "        coherence_values.append(coherencemodel.get_coherence())\n",
        "        perplexity_values.append(model.log_perplexity(corpus_TFIDF))\n",
        "        topic_n_list.append(num_topics)\n",
        "\n",
        "    for t, c, p in zip(topic_n_list, coherence_values, perplexity_values) :\n",
        "        print(\"topic_n={}, coherence : {}, perplexity : {}\".format(t,c,p))\n",
        "\n",
        "    return corpus_TFIDF, id2word, model_list, coherence_values, perplexity_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rG0tNlbE3dTy"
      },
      "outputs": [],
      "source": [
        "from gensim import corpora\n",
        "from gensim.models import LdaModel, TfidfModel\n",
        "\n",
        "tokenized_docs = get_news()\n",
        "corpus, id2word, model_list, coherence_values, perplexity_values = compute_coherence_perplexity(tokenized_docs, start=10, end=30, step=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4IRszRST5FZe"
      },
      "outputs": [],
      "source": [
        "lda_model = model_list[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LyTD0xfM-vdI"
      },
      "outputs": [],
      "source": [
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "\n",
        "pyLDAvis.enable_notebook()\n",
        "vis = gensimvis.prepare(lda_model, corpus, id2word, mds='tsne')\n",
        "pyLDAvis.display(vis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcIVqBHFg9WB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "e508c96042fd7b3aa969c1a8875668ac50b0a6c54de6b2bab6d59ac763cd3db2"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
